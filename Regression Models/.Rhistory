w = c(2, 1, 3, 1)
sum(w*((x-0.300)^2))
sum(w*((x-1.077)^2))
sum(w*((x-0.1471)^2))
sum(w*((x-0.0025)^2))
sum(w*((x-0.300)^2))
sum(w*((x-1.077)^2))
sum(w*((x-0.1471)^2))
sum(w*((x-0.0025)^2))
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
data = rbind(x,y)
data
data = cbind(x,y)
data
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
k = cbind(x,y)
k
lm(k$y~k$x, data =k)
k = as.dataframe(cbind(x,y))
k = as.data.frame(cbind(x,y))
k
lm(k$y~k$x, data =k)
summary(lm(k$y~k$x, data =k))
lm(k$y~k$x-1, data =k)
plot(k)
plot(k$x,k$x,pch=19,col="blue")
m = lm(k$y~k$x-1, data =k)
abline(m)
m = lm(k$y~k$x, data =k)
abline(m)
data(mtcars)
mcars$mpg
data(mtcars)
mcars$mpg
mtcars$mpg
mtcars$weight
summary(mtcars)
mtcars$wt
plot(k$x,k$y,pch=19,col="blue")
m = lm(k$y~k$x-1, data =k)
abline(m)
m = lm(k$y~k$x, data =k)
abline(m)
plot(mtcars$wt,k$mpg,pch=19,col="blue")
plot(k$mpg,mtcars$wt,pch=19,col="blue")
plot(mtcars$mpg,mtcars$wt,pch=19,col="blue")
plot(mtcars$wt,mtcars$mpg,pch=19,col="blue")
m = lm(mtcars$mpg~mtcars$wt, data=mtcars)
abline(m)
m.summary
m$summary
m
m = lm((mtcars$mpg~mtcars$wt-1), data=mtcars)
m
abline(m)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
s = sd(x)
m = mean(x)
(8.58-m)/s
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
k = as.data.frame(cbind(x,y))
k
plot(k$x,k$y,pch=19,col="blue")
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
k = as.data.frame(cbind(x,y))
k
plot(k$x,k$y,pch=19,col="blue")
m = lm(k$y~k$x-1, data =k)
m
abline(m)
m = lm(k$y~k$x, data =k)
abline(m)
m
x = dnorm(x, mean=0, sd=1)
x
summary(x)
x = norm(x, mean=0, sd=1)
x = rnorm(1:10, mean=0, sd=1)
summary(x)
y = rnorm(1:10, mean=0, sd=1)
summary(y)
k = as.data.frame(cbind(x,y))
k
m = lm(k$y~k$x, data =k)
abline(m)
m
m = lm((k$y~k$x)-1, data =k)
m = lm(k$y~k$x-1, data =k)
abline(m)
m
y <- galton$child
x <- galton$parent
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
rbind(c(beta0, beta1), coef(lm(y ~ x)))
beta1
beta0
y <- galton$child
x <- galton$parent
lm(y ~ x)
lm(x ~ y)
cor(y,x)
0.6463/0.3256
(0.6463/0.3256)
cor(y,x)
var(y)/var(x)
sd(y)/sd(x)
2*sd(y)/sd(x)
(2*sd(y))/sd(x)
x-0.573
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
x-0.573
(x-0.573)^2
sum((x-0.573)^2)
sum((x-0.8)^2)
sum((x-0.36)^2)
sum((x-0.44)^2)
data(diamond)
library(UsingR)
data(diamond)
plot(diamond$carat, diamond$price,
xlab = "Mass (carats)",
ylab = "Price (SIN $)",
bg = "lightblue",
col = "black", cex = 1.1, pch = 21,frame = FALSE)
abline(lm(price ~ carat, data = diamond), lwd = 2)
data(mtcars)
mtcars$mpg
mtcars$wt
m = lm(mtcars$mpg~mtcars$wt, data=mtcars)
m$residuals^2
sum(m$residuals^2)
m1 = lm(mtcars$mpg~mtcars$wt, data=mtcars)
sum(m1$residuals^2)
m2 = lm((mtcars$mpg~mtcars$wt-1), data=mtcars)
sum(m2$residuals^2)
m2/m1
str(m2)
m1 = lm(mtcars$mpg~mtcars$wt, data=mtcars)
x = sum(m1$residuals^2)
m2 = lm((mtcars$mpg~mtcars$wt-1), data=mtcars)
y = sum(m2$residuals^2)
y/x
x/y
m1
m2
summary(m1)$coefficients
summary(m2)$coefficients
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y ~ x);
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]
data(swiss);
str(swiss)
library(datasets);
data(swiss);
str(swiss)
library(caret)
data(mtcars)
head(mtcars)
model = lm(mpg ~ cyl+wt, data = mtcars)
summary(model)
model = lm(mpg ~ factor(cyl)+wt, data = mtcars)
summary(model)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
m = lm(y~x)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
m = lm(y~x)
hat(m)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
model5 <- lm(y~x)
hatvalues(model5)[1:5]
dfbeta(model5)[5,2]
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
model5 <- lm(y~x)
hatvalues(model5)[1:5]
dfbetas(model5)[5,2]
mtcars
names(mtCars)
names(mtcars)
str(mtcars)
mtcars$am = as.factor(mtcars$am)
str(mtcars)
df = mtcars
df$am = as.factor(df$am)
df$am = revalue(df$am, c("0"="Automatic", "1"="Manual"))
library(plyr)
library(ggplot2)
library(gridExtra)
library(caret)
data(mtcars)
library(plyr)
set.seed(3433)
str(mtcars)
# mtcars
df = mtcars
df$am = as.factor(df$am)
df$am = revalue(df$am, c("0"="Automatic", "1"="Manual"))
str(mtcars)
str(df)
?mtcars
str(mtcars)
df = mtcars
str(df)
df$am = as.factor(df$am)
str(df)
df$am = as.factor(df$am, c("0"="Automatic", "1"="Manual"))
df = mtcars
str(df)
df$am = as.factor(df$am)
str(df)
mtcars$am
df$am
str(df)
df$am
df$am = revalue(df$am, c("0"="Automatic", "1"="Manual"))
mtcars$am
df$am
inTrain = createDataPartition(y=df$am, p=0.75, list=FALSE)
dfT = dfTrain[inTrain,]
dfT = df[inTrain,]
dfV = df[-inTrain,]
plotmatrix(dfT) # plotting all of the variables
plotmatrix(dfT) # plotting all of the variables
library(GGally)
install.packages("GGally")
install.packages("GGally")
ggpairs(dfT,
upper = list(params = c(size = 10)),
lower = list(continuous = "smooth", params = c(method = "loess", fill = "blue"))
)
library(GGally)
ggpairs(dfT,
upper = list(params = c(size = 10)),
lower = list(continuous = "smooth", params = c(method = "loess", fill = "blue"))
)
ggpairs(data=dfT, # data.frame with variables
title="tips data", # title of the plot
colour = "am")
ggpairs(data=dfT, # data.frame with variables
columns=c("mpg","am","cyl"), # columns to plot, default to all.
title="tips data", # title of the plot
colour = "am")
ggpairs(data=dfT, # data.frame with variables
columns=c("mpg","am"), # columns to plot, default to all.
title="tips data", # title of the plot
colour = "am")
ggpairs(data=dfT, # data.frame with variables
columns=c("mpg","am","cyl"), # columns to plot, default to all.
title="tips data", # title of the plot
colour = "am")
columns=c("MilePerGallon","Transmission","NumOfCyclinders","HorsePower","Weight"), # columns to plot, default to all.
title="tips data", # title of the plot
ggpairs(data=dfT, # data.frame with variables
columns=c("MilePerGallon","Transmission","NumOfCyclinders","HorsePower","Weight"), # columns to plot, default to all.
title="tips data", # title of the plot
colour = "am")
ggpairs(data=dfT, # data.frame with variables
columns=c("MilePerGallon","Transmission","NumOfCyclinders","HorsePower","Weight"), # columns to plot, default to all.
title="tips data", # title of the plot
colour = "Transmission")
df = mtcars
df = rename(mtcars, c("mpg"="MilePerGallon", "cyl"="NumOfCyclinders", "disp"="Displacement", "hp"="HorsePower", "drat"="RearAxleRatio", "wt"="Weight","am"="Transmission","qsec"="qsec","vs"="V/S", "gear"="NumOfFowardGears","carb"="NumOfCarburetors"))
df$Transmission = as.factor(df$Transmission)
df$Transmission = revalue(df$Transmission, c("0"="Automatic", "1"="Manual"))
inTrain = createDataPartition(y=df$Transmission, p=0.75, list=FALSE)
dfT = df[inTrain,]
dfV = df[-inTrain,]
# Exploration
ggpairs(data=dfT, # data.frame with variables
columns=c("MilePerGallon","Transmission","NumOfCyclinders","HorsePower","Weight"), # columns to plot, default to all.
title="tips data", # title of the plot
colour = "Transmission")
ggplot(dfT, aes(x=Transmission)) + geom_histogram(aes(y=..density..),binwidth=.2, colour="black", fill="white") +
ggtitle("Transmission Frequency in Training Set") +
geom_density(alpha=.2, fill="#FF6666")
b1 = ggplot(dfT, aes(x=Transmission, y=MilesPerGallon)) + geom_boxplot()
b2 = ggplot(dfT, aes(x=Tranmission, y=HorsePower)) + geom_boxplot()
b3 = ggplot(dfT, aes(x=Tranmission, y=NumOfCyclinders)) + geom_boxplot()
b4 = ggplot(dfT, aes(x=Tranmission, y=Weight)) + geom_boxplot()
grid.arrange(b1, b2, b3, b4, main = "Mileage ") # notice the outliers; hoping PCA processing should smooth that out.
b1 = ggplot(dfT, aes(x=Transmission, y=MilePerGallon)) + geom_boxplot()
b2 = ggplot(dfT, aes(x=Tranmission, y=HorsePower)) + geom_boxplot()
b3 = ggplot(dfT, aes(x=Tranmission, y=NumOfCyclinders)) + geom_boxplot()
b4 = ggplot(dfT, aes(x=Tranmission, y=Weight)) + geom_boxplot()
grid.arrange(b1, b2, b3, b4, main = "Mileage ") # notice the outliers; hoping PCA processing should smooth that out.
b1 = ggplot(dfT, aes(x=Transmission, y=MilePerGallon)) + geom_boxplot()
b2 = ggplot(dfT, aes(x=Transmission, y=HorsePower)) + geom_boxplot()
b3 = ggplot(dfT, aes(x=Transmission, y=NumOfCyclinders)) + geom_boxplot()
b4 = ggplot(dfT, aes(x=Transmission, y=Weight)) + geom_boxplot()
grid.arrange(b1, b2, b3, b4, main = "Mileage ") # notice the outliers; hoping PCA processing should smooth that out.
b1 = ggplot(dfT, aes(x=Transmission, y=MilePerGallon)) + geom_boxplot()
b2 = ggplot(dfT, aes(x=HorsePower, y=MilePerGallon)) + geom_boxplot()
b3 = ggplot(dfT, aes(x=NumOfCyclinders, y=MilePerGallon)) + geom_boxplot()
b4 = ggplot(dfT, aes(x=Weight, y=MilePerGallon)) + geom_boxplot()
grid.arrange(b1, b2, b3, b4, main = "Mileage ") # notice the outliers; hoping PCA processing should smooth that out.
b1 = ggplot(dfT, aes(x=Transmission, y=MilePerGallon)) + geom_line() + geom_point()
b2 = ggplot(dfT, aes(x=HorsePower, y=MilePerGallon)) + geom_line() + geom_point()
b3 = ggplot(dfT, aes(x=NumOfCyclinders, y=MilePerGallon)) + geom_line() + geom_point()
b4 = ggplot(dfT, aes(x=Weight, y=MilePerGallon)) + geom_line() + geom_point()
grid.arrange(b1, b2, b3, b4, main = "Mileage ") # notice the outliers; hoping PCA processing should smooth that out.
cols = c("MilePerGallon", "NumOfCyclinders", "Displacement", "HorsePower","RearAxleRatio", "Weight","qsec","V/S", "NumOfFowardGears","NumOfCarburetors"))
## PCA Processing
cols = c("MilePerGallon", "NumOfCyclinders", "Displacement", "HorsePower","RearAxleRatio", "Weight","qsec","V/S", "NumOfFowardGears","NumOfCarburetors")
preProc = preProcess(dfT[,cols], method="pca", thresh=0.80)
trainPC = predict(preProc, dfT[,cols])
model = lm(Transmission ~ MilePerGallon, data = dfT)
model = lm(Transmission ~ ., data = dfT)
df$Tranmission = as.numeric(df$Transmission)
df = mtcars
df = rename(mtcars, c("mpg"="MilePerGallon", "cyl"="NumOfCyclinders", "disp"="Displacement", "hp"="HorsePower", "drat"="RearAxleRatio", "wt"="Weight","am"="Transmission","qsec"="qsec","vs"="V/S", "gear"="NumOfFowardGears","carb"="NumOfCarburetors"))
df$Transmission = as.factor(df$Transmission)
df$Transmission = revalue(df$Transmission, c("0"="Automatic", "1"="Manual"))
inTrain = createDataPartition(y=df$Transmission, p=0.75, list=FALSE)
dfT = df[inTrain,]
dfV = df[-inTrain,]
df = mtcars
df = rename(mtcars, c("mpg"="MilePerGallon", "cyl"="NumOfCyclinders", "disp"="Displacement", "hp"="HorsePower", "drat"="RearAxleRatio", "wt"="Weight","am"="Transmission","qsec"="qsec","vs"="V/S", "gear"="NumOfFowardGears","carb"="NumOfCarburetors"))
inTrain = createDataPartition(y=df$Transmission, p=0.75, list=FALSE)
dfT = df[inTrain,]
dfV = df[-inTrain,]
# Exploration
# Here's a visual cue on how the different variables affect the mileage.
ggpairs(data=dfT, # data.frame with variables
columns=c("MilePerGallon","Transmission","NumOfCyclinders","HorsePower","Weight"), # columns to plot, default to all.
title="tips data", # title of the plot
colour = "Transmission")
df$Transmission = as.factor(df$Transmission)
df$Transmission = revalue(df$Transmission, c("0"="Automatic", "1"="Manual"))
inTrain = createDataPartition(y=df$Transmission, p=0.75, list=FALSE)
dfT = df[inTrain,]
dfV = df[-inTrain,]
# Exploration
# Here's a visual cue on how the different variables affect the mileage.
ggpairs(data=dfT, # data.frame with variables
columns=c("MilePerGallon","Transmission","NumOfCyclinders","HorsePower","Weight"), # columns to plot, default to all.
title="tips data", # title of the plot
colour = "Transmission")
dfT$Transmission = as.numeric(levels(dfT[,"Transmission"])
dfT$Transmission = as.numeric(levels(dfT[,"Transmission"]))
dfT$Transmission = as.numeric(levels(dfT[,"Transmission"]))
dfT$Transmission = as.numeric(dfT[,"Transmission"])
# mtcars
df = mtcars
df = rename(mtcars, c("mpg"="MilePerGallon", "cyl"="NumOfCyclinders", "disp"="Displacement", "hp"="HorsePower", "drat"="RearAxleRatio", "wt"="Weight","am"="Transmission","qsec"="qsec","vs"="V/S", "gear"="NumOfFowardGears","carb"="NumOfCarburetors"))
df$Transmission = as.factor(df$Transmission)
df$Transmission = revalue(df$Transmission, c("0"="Automatic", "1"="Manual"))
inTrain = createDataPartition(y=df$Transmission, p=0.75, list=FALSE)
dfT = df[inTrain,]
dfV = df[-inTrain,]
# Exploration
# Here's a visual cue on how the different variables affect the mileage.
ggpairs(data=dfT, # data.frame with variables
columns=c("MilePerGallon","Transmission","NumOfCyclinders","HorsePower","Weight"), # columns to plot, default to all.
title="tips data", # title of the plot
colour = "Transmission")
Clearly, manual tranmission cars have more mileage than automatic tranmission. In fact, the mileage for automatic tranmission range from ~10 miles to 23 miles, while manual transmission ranges from ~ 17 miles to 35 miles.More over, other factors like number of cyclinders, horsepower, weight etc also have a negative impact on Mileages. Interestingly, this negative impact is higher for manual tranmission than automatic transmission.
#~~~~~ frequency
ggplot(dfT, aes(x=Transmission)) + geom_histogram(aes(y=..density..),binwidth=.2, colour="black", fill="white") +
ggtitle("Transmission Frequency in Training Set") +
geom_density(alpha=.2, fill="#FF6666")
#~~~ boxplot
b1 = ggplot(dfT, aes(x=Transmission, y=MilePerGallon)) + geom_line() + geom_point()
b2 = ggplot(dfT, aes(x=HorsePower, y=MilePerGallon)) + geom_line() + geom_point()
b3 = ggplot(dfT, aes(x=NumOfCyclinders, y=MilePerGallon)) + geom_line() + geom_point()
b4 = ggplot(dfT, aes(x=Weight, y=MilePerGallon)) + geom_line() + geom_point()
grid.arrange(b1, b2, b3, b4, main = "Mileage Perspective") # notice the outliers; hoping PCA processing should smooth that out.
## PCA Processing
cols = c("MilePerGallon", "NumOfCyclinders", "Displacement", "HorsePower","RearAxleRatio", "Weight","qsec","V/S", "NumOfFowardGears","NumOfCarburetors")
preProc = preProcess(dfT[,cols], method="pca", thresh=0.80)
trainPC = predict(preProc, dfT[,cols])
dfT$Transmission = as.numeric(dfT[,"Transmission"])
model = lm(Transmission ~ ., data = dfT)
plot(resid(model))
abline(h=0)
summary(model)
deviance(model) # residual sum of squares
confint(model)
anova(model)
## Training the model with just "MilePerGallon", "NumOfCyclinders", "HorsePower","Weight"
cols = c("MilePerGallon", "NumOfCyclinders", "HorsePower","Weight")
preProc = preProcess(dfT[,cols], method="pca", thresh=0.80)
trainPC = predict(preProc, dfT[,cols])
## Training the model with all variables
dfT$Transmission = as.numeric(dfT[,"Transmission"])
model = lm(Transmission ~ ., data = dfT)
plot(resid(model))
abline(h=0)
summary(model)
dfT$Transmission = as.numeric(dfT[,"Transmission"])
model = lm(Transmission ~ cols, data = dfT)
dfT$Transmission = as.numeric(dfT[,"Transmission"])
model = lm(Transmission ~ MilePerGallon+NumOfCyclinders+HorsePower+Weight, data = dfT)
plot(resid(model))
abline(h=0)
summary(model)
install.packages("pdflatex")
install.packages("MiKetx")
library("ElemStatLearn", lib.loc="~/R/win-library/3.1")
data(vowel.train)
data(vowel.test)
vowel.test
names(vowel.test)
names(vowel.train)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
str(vowel.train$y)
set.seed(33833)
mod1 <- train(y ~.,method="gbm",data=vowel.train)
library("caret", lib.loc="~/R/win-library/3.1")
mod1 <- train(y ~.,method="gbm",data=vowel.train)
mod2 <- train(y ~.,method="rf", data=vowel.train)
pred1 <- predict(mod1,vowel.test)
pred2 <- predict(mod2,vowel.test)
pred1
sqrt(sum((pred1-vowel.test$wage)^2))
sqrt(sum((pred1-vowel.test$y)^2))
names(vowel.test)
str(vowel.test)
mod1 <- train(y ~.,method="gbm",data=vowel.train)
pred1
data(vowel.test)
names(vowel.test)
str(vowel.test)
sqrt(sum((pred1-vowel.test$y)^2))
str(vowel.test)
str(pred1)
str(as.numeric(pred1))
sqrt(sum((as.numeric(pred1)-vowel.test$y)^2))
pred1 <- predict(mod1,vowel.test)
pred2 <- predict(mod2,vowel.test)
str(as.numeric(pred1))
str(vowel.test)
library(gbm)
library(gbm)
set.seed(33833)
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
str(vowel.train$y)
mod1 <- train(y ~.,method="gbm",data=vowel.train)
mod2 <- train(y ~.,method="rf", data=vowel.train)
sqrt(sum(pred1-vowel.test$y)^2))
sqrt(sum((pred1-vowel.test$y)^2))
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
# Create a building data set and validation set
library(ISLR); data(Wage); library(ggplot2); library(caret);
install.packages("ISLR")
library(ISLR); data(Wage); library(ggplot2); library(caret);
Wage <- subset(Wage,select=-c(logwage))
# Create a building data set and validation set
inBuild <- createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
validation <- Wage[-inBuild,]; buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage,
p=0.7, list=FALSE)
str(training)
training <- buildData[inTrain,]; testing <- buildData[-inTrain,]
str(training)
sqrt(sum((as.numeric(pred1)-as.numeric(vowel.test$y)^2)))
library(gbm)
set.seed(33833)
data(vowel.train)
data(vowel.test)
str(vowel.train$y)
mod1 <- train(y ~.,method="gbm",data=vowel.train)
mod2 <- train(y ~.,method="rf", data=vowel.train)
pred1 <- predict(mod1,vowel.test)
pred2 <- predict(mod2,vowel.test)
sqrt(sum((pred1)-(vowel.test$y)^2)))
sqrt(sum((pred1)-(vowel.test$y)^2))
vowel.test$y
sqrt(sum((pred1-vowel.test$y)^2))
sqrt(sum((pred2-vowel.test$y)^2))
confusionMatrix(pred1,pred2)
library(gbm)
set.seed(33833)
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
str(vowel.train$y)
mod1 <- train(y ~.,method="gbm",data=vowel.train)
mod2 <- train(y ~.,method="rf", data=vowel.train)
pred1 <- predict(mod1,vowel.test)
pred2 <- predict(mod2,vowel.test)
pred1-vowel.test$y
mod1
mod2
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
library("caret", lib.loc="~/R/win-library/3.1")
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
library("e1071", lib.loc="~/R/win-library/3.1")
model = svm(CompressiveStrength ~ ., data = training)
pred = predict(model, testing)
sqrt((sum(pred - testing$CompressiveStrength)^2)/length(pred))
