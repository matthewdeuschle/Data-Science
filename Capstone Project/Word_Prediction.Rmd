---
title: "Word Prediction Capstone Project"
output: html_document 
---


```{r setupEnvironment, include=FALSE, cache=FALSE}
library(plyr)
library(ggplot2)
library(gridExtra)
library(caret)
library(tm)
library(RWeka)
library(R.utils)
library(stringi)
library(stringr)
library(SnowballC)
library(RColorBrewer)
library(magrittr)
library(wordcloud)
library(textcat)
library(xtable)

blackList = read.csv("./Data/Terms-to-Block.csv", skip=4)
blackList = blackList[,2]
blackList = gsub(",","",blackList)

source("./Utilities.R")
```

### Synopsis
Pervasive computing has seen its share of growth with the advent of smart phones and tablets. These devices provide a pathway to bring intelligence closer to a person. One such intelligence is the concept of predicting words as you type on these devices. It could be as simple as typing a text message or as complex as writing a word document. Regardless of the scenario, the intelligence of predicting words based on what the user has already typed will immensly increase the usability factor, when it comes to using these devices.

The goal of this project is to create such a predictive model, which by learning existing corpus, can provide suggestions to users as they use their smart phone or tablet keypads. The final model will be implemented in the form of an attractive Shiny app and will be avalable for the general public. However, to get to the final model, we'll have to traverse through the following stages/steps:

<ol>
  <li>Model Creation
    <ol>
      <li>Prelimiary Preparation</li>
      <li>Inspect Provided Dataset</li>
      <li>PreProcessing</li>
      <li>Exploratory Analysis</li>
      <li>Training the Model and Predicting the Outcome</li>
      <li>Analyze Accuracy of the Model</li>
    </ol>
  </li>
  <li>Applying the Model</li> 
</ol>

The dataset for this project is available at this <a href="https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip" target="_blank">location</a>

### Model Creation
Before a model can be created, the given dataset should be prepared, consumed and explored. Below are the corresponding code snippets and explanations.


#### Preliminary Preparation
The provided Corpus consist of three files which contains English text. We'll use the <a href="http://cran.r-project.org/web/packages/tm/index.html" target="_blank">tm</a> R package to read data from these files.
<ol>
  <li>en_US.blogs.txt   - contains text from various US blogs.</li>
  <li>en_US.news.txt    - contains text from various news sites</li>
  <li>en_US.twitter.txt - contains text from numerous twitter feeds</li>
</ol>

```{r readBlogProp, echo=FALSE, eval=TRUE}
fileLineNum = CalculateTextFileLines("./Data/Corpus/en_US.blogs.txt")
fileWordCount = CountWordsInTextFile("./Data/Corpus/en_US.blogs.txt")
```

#####Some interesting facts about the English blog corpus - en_US.blogs.txt
```{r displayBlogProp, echo=FALSE, results='asis'}
df  = data.frame("Number of Lines" = (fileLineNum),
"Total Word Count (Millions)" = (fileWordCount/1E6))
colnames(df) = c("Number of Lines", "Total Word Count (Millions)")
print(xtable(df, display = c("s","f","f")),
      type="html")
```


```{r readTwitterProp, echo=FALSE, eval=TRUE}
fileLineNum = CalculateTextFileLines("./Data/Corpus/en_US.twitter.txt")
fileWordCount = CountWordsInTextFile("./Data/Corpus/en_US.twitter.txt")
```

#####Some interesting facts about the English twitter corpus - en_US.twitter.txt
```{r displayTwitterProp, echo=FALSE, results='asis'}
df  = data.frame("Number of Lines" = (fileLineNum),
"Total Word Count" = (fileWordCount/1E6))
colnames(df) = c("Number of Lines", "Total Word Count (Millions)")
print(xtable(df, display = c("s","f","f")),
      type="html")
```

```{r readNewsProp, echo=FALSE, eval=TRUE}
fileLineNum = CalculateTextFileLines("./Data/Corpus/en_US.news.txt")
fileWordCount = CountWordsInTextFile("./Data/Corpus/en_US.news.txt")
```

#####Some interesting facts about the English News corpus - en_US.news.txt
```{r displayNewsProp, echo=FALSE, results='asis'}
df  = data.frame("Number of Lines" = (fileLineNum),
"Total Word Count" = (fileWordCount/1E6))
colnames(df) = c("Number of Lines", "Total Word Count (Millions)")
print(xtable(df, display = c("s","f","f")),
      type="html")
```


##### Sampling
Considering the amount of data in each of the contents (blog, news, twitter), it doesn't seem practical to explore the entire content. Hence I'll randomly select 1% from each content that would represent the larger content. Following is the exploration of the subset.

```{r sampling, echo=FALSE, results='hide', cache=TRUE}
percentageToSample = 1
outputTextFileDirectory = "./Data/OutputData"

# Create random sample of corpus and save it
CreateRandomCorpusSample("./Data/Corpus",
                                  percentageToSample,
                                  outputTextFileDirectory) 


```
